{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_Hello_World.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBjoqGbtL+YSLzcNENCdir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datasith/ML-Notebooks-TensorFlow/blob/main/TensorFlow_Hello_World.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A First Shot at Deep Learning with TensorFlow\n",
        "\n",
        "In this notebook, we are going to take a baby step into the world of deep learning using TensorFlow. There are tons of notebooks out there that teach you the fundamentals in detail, so the idea here is to give you a high level introduction to deep learning and TensorFlow. Therefore, this notebook is targeting beginners but it can also serve as a review for more experienced developers.\n",
        "\n",
        "After completion of this notebook, you are expected to know the basic components of training a basic neural network with TensorFlow. I have also left a couple of exercises towards the end with the intention of encouraging more research and practise of your deep learning skills. \n",
        "\n",
        "---\n",
        "**Author:** Cisco Zabala ([@datasith](https://twitter.com/datasith) | [LinkedIn](https://www.linkedin.com/in/datasith/) | [Kaggle](https://kaggle.com/thedatasith) | [GitHub](https://github.com/datasith))\n",
        "\n",
        "*Based on the work by Elvis Saravia ([Twitter](https://twitter.com/omarsar0) | [LinkedIn](https://www.linkedin.com/in/omarsar/)) on GitHub: [ML Notebooks](https://github.com/dair-ai/ML-Notebooks)*"
      ],
      "metadata": {
        "id": "MQk4TUMxCz5h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkzttrQCwaSQ"
      },
      "source": [
        "## Importing the libraries\n",
        "\n",
        "Like with any other programming exercise, the first step is to import the necessary libraries. As we are going to be using Google Colab to program our neural network, we need to install and import the necessary TensorFlow libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuhJIaeXO2W9",
        "outputId": "bb2e7b4c-35b4-480d-dbae-ccfb75f27db1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## The usual imports\n",
        "import tensorflow as tf\n",
        "\n",
        "## print out the tensorflow version used\n",
        "print(tf.__version__)\n",
        "\n",
        "## print out any available GPU devices\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a2C_nneO_wp"
      },
      "source": [
        "## The Neural Network\n",
        "\n",
        "![](https://raw.githubusercontent.com/datasith/ML-Notebooks-TensorFlow/main/img/TensorFlow_Hello_World/model-nn.png)\n",
        "*Source: Elvis S. (2022)*\n",
        "\n",
        "Before building and training a neural network the first step is to process and prepare the data. In this notebook, we are going to use syntethic data (i.e., fake data) so we won't be using any real world data. \n",
        "\n",
        "For the sake of simplicity, we are going to use the following input and output pairs converted to tensors, which is how data is typically represented in the world of deep learning. The x values represent the input of dimension `(6,1)` and the y values represent the output of similar dimension. The example is taken from this [tutorial](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb). \n",
        "\n",
        "The objective of the neural network model that we are going to build and train is to automatically learn patterns that better characterize the relationship between the `x` and `y` values. Essentially, the model learns the relationship that exists between inputs and outputs which can then be used to predict the corresponding `y` value for any given input `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWFtgUX85iwO"
      },
      "source": [
        "## our data in tensor form\n",
        "x = tf.constant([[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]], dtype=tf.float32)\n",
        "y = tf.constant([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]], dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQUjR_95z5J",
        "outputId": "48d3088a-79d5-4b03-edca-556e62863fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## print size of the input tensor\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([6, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CJXO5WX1QtQ"
      },
      "source": [
        "## The Neural Network Components\n",
        "As said earlier, we are going to first define and build out the components of our neural network before training the model.\n",
        "\n",
        "### Model\n",
        "\n",
        "Typically, when building a neural network model, we define the layers and weights which form the basic components of the model. Below we show an example of how to define a hidden layer named `layer1` with size `(1, 1)`. For the purpose of this tutorial, we won't explicitly define the `weights` and allow the built-in functions provided by TensorFlow to handle that part for us. By the way, we use a single `tf.keras.layers.Dense(...)` layer so as to apply a linear transformation ($y = xA^T + b$) to the data that was provided as its input. We ignore the bias for now by setting `use_bias=False`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Ii5JRz3Jud"
      },
      "source": [
        "## Neural network with 1 hidden layer\n",
        "layer1 = tf.keras.layers.Dense(units=1,\n",
        "                               input_shape=[1],\n",
        "                               name=\"layer1\",\n",
        "                               activation=\"linear\",\n",
        "                               use_bias=False)\n",
        "model = tf.keras.Sequential([layer1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTWYD4aMBXQ"
      },
      "source": [
        "### Loss and Optimizer\n",
        "The loss function, `tf.keras.losses.MeanSquaredError()`, is in charge of letting the model know how good it has learned the relationship between the input and output. The optimizer (in this case a `SGD`) primary role is to compute the gradients after each forward pass for minimizing (lowering) the loss value by adjusting the model's single weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hglFpejArxx"
      },
      "source": [
        "# ## loss function\n",
        "criterion = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# ## optimizer algorithm\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKj6jvZTUtGh"
      },
      "source": [
        "## Training the Neural Network Model\n",
        "We have all the components we need to train our model. Below is the code used to train our model. \n",
        "\n",
        "In simple terms, we train the model by feeding it the input and output pairs for a series of rounds (i.e., `epochs`). After a series of forward and backward passes, the model learns—if everything goes well—the best or one of the best relationship between x and y values. This is evidenced by the decrease in the computed `loss`. For a more detailed explanation of this code check out this [tutorial](https://developers.google.com/codelabs/tensorflow-1-helloworld#2) by the folks over at Google. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeOr9i-aBzRv",
        "outputId": "358b6cca-1e8f-4480-a14e-4727524ab46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## training\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=criterion)\n",
        "\n",
        "model.fit(x, y, epochs=150)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.5645\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5645\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5645\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5645\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5645\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5645\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5645\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5645\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5645\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5645\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5645\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5645\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5645\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5645\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5645\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5645\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5645\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5645\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5645\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5645\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5645\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5645\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5645\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5645\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5645\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33a6447c50>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp50Q7J0Xkiw"
      },
      "source": [
        "## Testing the Model\n",
        "After training the model we have the ability to test the model predictive capability by passing it an input. Below is a simple example of how you could achieve this with our model. The result we obtained aligns with the results obtained in this [notebook](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb), which inspired this entire tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1odfZpGFoBi",
        "outputId": "2bdbeba2-bf8d-42be-e477-3a20c09f89c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## test the model\n",
        "sample = tf.constant([10.0], dtype=tf.float32)\n",
        "predicted = model(sample)\n",
        "print(predicted.numpy().item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.096769332885742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozX4V1GhPLyr"
      },
      "source": [
        "## Final Words\n",
        "\n",
        "Congratulations! In this tutorial you learned how to train a simple neural network using TensorFlow. We used the fundamental components that make up a neural network model such as the Sequential class, Dense layer, optimizer, and loss function. We then trained the model and tested its predictive capabilities. Having seen this, you are well on your way to become more knowledgeable about deep learning and TensorFlow. I have provided a bunch of references below if you are interested in practicing and learning more. \n",
        "\n",
        "*I would like to thank Laurence Moroney for sharing the [resources](https://github.com/lmoroney/dlaicourse/) used for his Deep Learning course (available on MOOC platforms). They served as an inspiration for this tutorial.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAABGiMHeDOr"
      },
      "source": [
        "## Exercises\n",
        "- What happens to the loss if we include a bias term (i.e., `use_bias` parameter)?\n",
        "- Add more examples in the input and output tensors. In addition, try to change the dimensions of the data, say by adding an extra value in each array. What needs to be changed to successfully train the network with the new data?\n",
        "- The model converged really fast, which means it learned the relationship between x and y values after a couple of iterations. Do you think it makes sense to continue training? How would you automate the process of stopping the training after the model loss doesn't subtantially change?\n",
        "- In our example, we used a single hidden layer. Try to take a look at the PyTorch documentation to figure out what you need to do to get a model with more layers. What happens if you add more hidden layers?\n",
        "- We did not discuss the learning rate (`lr-0.001`) and the optimizer in great detail. Check out the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) to learn more about what other optimizers you can use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-o4w9vpPHZz"
      },
      "source": [
        "## References\n",
        "- [The Hello World of Deep Learning with Neural Networks](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb)\n",
        "- [A Simple Neural Network from Scratch with PyTorch and Google Colab](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0)\n",
        "- [TensorFlow Official Docs](https://www.tensorflow.org/api_docs)"
      ]
    }
  ]
}